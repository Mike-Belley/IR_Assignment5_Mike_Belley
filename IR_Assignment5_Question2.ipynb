{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "w9v74MT4CKi6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ymqEG5DB9R6",
        "outputId": "acf09ab7-d0f1-4c3d-cd8b-4f7165f56916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import string\n",
        "\n",
        "import csv\n",
        "\n",
        "import math\n",
        "\n",
        "from post_parser_record import PostParserRecord\n",
        "post_reader = PostParserRecord(\"Posts_Coffee.xml\")\n",
        "\n",
        "from scipy.spatial import distance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BM25"
      ],
      "metadata": {
        "id": "FwLJGir1CN5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create BM25"
      ],
      "metadata": {
        "id": "hWKikAcDCiVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_bm25():\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_words.add('p')\n",
        "\n",
        "  # initialize tf, idf, document length dictionaries, total docs, and total tokens\n",
        "  tf_dict = {}\n",
        "  idf_dict = {}\n",
        "  doc_length = {}\n",
        "  total_docs = 0\n",
        "  total_tokens = 0\n",
        "\n",
        "  # calculate tf\n",
        "  # go thru all questions\n",
        "  for question_id in post_reader.map_questions:\n",
        "    question = post_reader.map_questions[question_id]\n",
        "\n",
        "    # tokenize the title and body and filter out the unwanted stuff\n",
        "    word_tokens = word_tokenize(question.title)\n",
        "    word_tokens += word_tokenize(question.body)\n",
        "    filtered_text = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "    # get the amount of terms in the doc\n",
        "    terms_amt = len(filtered_text)\n",
        "\n",
        "    # add doc length to dict, increment total docs and total tokens\n",
        "    doc_length[question_id] = terms_amt\n",
        "    total_docs += 1\n",
        "    total_tokens += terms_amt\n",
        "\n",
        "    # go thru all the strings in filtered_text\n",
        "    for word in filtered_text:\n",
        "      # if the word is already in the dictionary\n",
        "      if word in tf_dict.keys():\n",
        "        # if the questionid for the word is there, increment\n",
        "        if question_id in tf_dict[word].keys():\n",
        "          tf_dict[word][question_id] += (1 / terms_amt)\n",
        "\n",
        "        # if the questionid for the word is not there, initialize\n",
        "        else:\n",
        "          tf_dict[word][question_id] = (1 / terms_amt)\n",
        "\n",
        "      # if the word is not in the dictionary, initialize it\n",
        "      else:\n",
        "        tf_dict[word] = {question_id: (1 / terms_amt)}\n",
        "\n",
        "  # # go thru all the answers\n",
        "  # for answer_id in post_reader.map_just_answers:\n",
        "  #   answer = post_reader.map_just_answers[answer_id]\n",
        "\n",
        "  #   # tokenize the body and filter out the unwanted stuff\n",
        "  #   word_tokens = word_tokenize(answer.body)\n",
        "  #   filtered_text = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "  #   # get the amount of terms in the doc\n",
        "  #   terms_amt = len(filtered_text)\n",
        "\n",
        "  #   # add doc length to dict, increment total docs and total tokens\n",
        "  #   doc_length[answer_id] = terms_amt\n",
        "  #   total_docs += 1\n",
        "  #   total_tokens += terms_amt\n",
        "\n",
        "  #   # go thru all the strings in filtered_text\n",
        "  #   for word in filtered_text:\n",
        "  #     # if the word is already in the dictionary\n",
        "  #     if word in tf_dict.keys():\n",
        "  #       # if the answerid for the word is there, increment\n",
        "  #       if answer_id in tf_dict[word].keys():\n",
        "  #         tf_dict[word][answer_id] += (1 / terms_amt)\n",
        "\n",
        "  #       # if the answerid for the word is not there, initialize\n",
        "  #       else:\n",
        "  #         tf_dict[word][answer_id] = (1 / terms_amt)\n",
        "\n",
        "  #     # if the word is not in the dictionary, initialize it\n",
        "  #     else:\n",
        "  #       tf_dict[word] = {answer_id: (1 / terms_amt)}\n",
        "    \n",
        "  \n",
        "  # calculate idf\n",
        "  # get total amount of docs\n",
        "  doc_amount = len(post_reader.map_questions)# + len(post_reader.map_just_answers)\n",
        "\n",
        "  # go thru all the words in tf dictionary and calculate idf\n",
        "  idf_dict = {word: (math.log( (doc_amount / len(tf_dict[word].keys())) , 2)) for word in tf_dict.keys()}\n",
        "\n",
        "  # return tf, idf, document length dictionaries and average length\n",
        "  return tf_dict, idf_dict, doc_length, total_tokens / total_docs\n",
        "\n",
        "# Process BM25\n",
        "tf_dict, idf_dict, doc_length, average_length = process_bm25()"
      ],
      "metadata": {
        "id": "OmgiSQLxCTJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a search and print the top results\n",
        "def bm_25_search(search_text, b, k1):\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_words.add('p')\n",
        "\n",
        "  # tokenize the body and filter out the unwanted stuff\n",
        "  word_tokens = word_tokenize(search_text)\n",
        "  search_words = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "  # initialize a dictionary to store document scores\n",
        "  doc_scores = {}\n",
        "\n",
        "  # go thru all the terms\n",
        "  for word in search_words:\n",
        "    # go thru all the docs for the word\n",
        "    for doc_id in tf_dict[word].keys():\n",
        "      # if the doc has not been score, initialize\n",
        "      if doc_id not in doc_scores.keys():\n",
        "        doc_scores[doc_id] = (idf_dict[word] * (((k1 + 1) * tf_dict[word][doc_id]) / ((k1 * ((1 - b) + (b * (doc_length[doc_id] / average_length)))) + tf_dict[word][doc_id])))\n",
        "      # if the doc has been scored, increment\n",
        "      else:\n",
        "        doc_scores[doc_id] += (idf_dict[word] * (((k1 + 1) * tf_dict[word][doc_id]) / ((k1 * ((1 - b) + (b * (doc_length[doc_id] / average_length)))) + tf_dict[word][doc_id])))\n",
        "\n",
        "  \n",
        "  # print(search_docs_rank)\n",
        "  # order by the values\n",
        "  sorted_by_value = dict(sorted(doc_scores.items(), key=lambda item: item[1], reverse=True))\n",
        "  print(\"Search Results for: \" + search_text)\n",
        "  for i in range(1,11):\n",
        "    # print(str(list(sorted_by_value.keys())[i-1]) + \"\\n\")\n",
        "    # print(str(list(sorted_by_value.values())[i-1]) + \"\\n\")\n",
        "    print(str(i) + \". \" + str(list(sorted_by_value.keys())[i-1]) + \" : \" + str(list(sorted_by_value.values())[i-1]))"
      ],
      "metadata": {
        "id": "ReRaQcRZCTtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results"
      ],
      "metadata": {
        "id": "ZYUcF4uUCmkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = 0.75\n",
        "k1 = 1.2\n",
        "\n",
        "bm_25_search(\"how to make espresso\", b, k1)\n",
        "\n",
        "bm_25_search(\"moka pot\", b, k1)\n",
        "\n",
        "bm_25_search(\"coffee caffeine\", b, k1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahhZSp4WCWpz",
        "outputId": "6e88cb27-5f65-47e1-b1f7-48442cdf2199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search Results for: how to make espresso\n",
            "1. 4404 : 1.2394862199905607\n",
            "2. 5526 : 1.178071422060564\n",
            "3. 5121 : 1.1312409263567955\n",
            "4. 4739 : 0.9872093529045914\n",
            "5. 4258 : 0.9264812157429019\n",
            "6. 2867 : 0.902578425382284\n",
            "7. 4175 : 0.8844814651156854\n",
            "8. 4406 : 0.8515163985919738\n",
            "9. 94 : 0.8384360248231166\n",
            "10. 3143 : 0.8384360248231166\n",
            "Search Results for: moka pot\n",
            "1. 5066 : 2.391156771453269\n",
            "2. 4500 : 2.2085357310940084\n",
            "3. 3381 : 1.9218586430695521\n",
            "4. 5070 : 1.5981155396408173\n",
            "5. 97 : 1.5530474695435768\n",
            "6. 4710 : 1.4572907573703662\n",
            "7. 122 : 1.4547910272975648\n",
            "8. 2024 : 1.384641997966383\n",
            "9. 4299 : 1.3686101594336146\n",
            "10. 5546 : 1.340349995237566\n",
            "Search Results for: coffee caffeine\n",
            "1. 2387 : 1.8587922238595915\n",
            "2. 204 : 1.8427245558778806\n",
            "3. 2358 : 1.5643365700272247\n",
            "4. 94 : 1.4154702530746701\n",
            "5. 2127 : 1.4154702530746701\n",
            "6. 3566 : 1.2804855192599771\n",
            "7. 29 : 1.2662177289375511\n",
            "8. 462 : 1.2578243525720696\n",
            "9. 3475 : 1.215776379839394\n",
            "10. 128 : 1.153389997394759\n"
          ]
        }
      ]
    }
  ]
}